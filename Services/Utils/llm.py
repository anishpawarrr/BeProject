"""
This module contains the LLM class which interacts with the Hugging Face Inference API.
"""

from os import getenv
from huggingface_hub import InferenceClient
from .llm_cache import LLMCache

class LLM():
    """
    Class to interact with the Hugging Face Inference API.
    Generates response for given prompts.
    """

    def __init__(self,
                 model: str = getenv("MODEL_PHI"),
                 max_tokens: int = 10000,
                 tempature: float = 0.1,
                 llm_cache: LLMCache = LLMCache()
                 ):
        """
        Constructor for the LLM class.

        Args:
            model (str): Model name to be used for inference.
            max_tokens (int): Maximum number of tokens to generate.
            tempature (float): Sampling temperature for generation.
            llm_cache (LLMCache): Cache object to store responses.
        """
        self.__client = InferenceClient(api_key=getenv("HF_LOGIN_TOKEN"))
        self.__model = model
        self.__max_tokens = max_tokens
        self.__tempature = tempature
        self.__llm_cache = llm_cache

    def generate_response(self,
                          user_prompt: str,
                          system_prompt: str = "",
                          context: str = "",
                          cache_enabled: bool = True
                          ) -> str:

        """
        Get response from LLM / Cache.

        Args:
            system_prompt (str): System prompt to be used.
            user_prompt (str): User prompt to be used.
            context (str): Context to be used.
            cache_enabled (bool): Flag to enable/disable cache.

        Returns:
            str: Response generated by the model / response present in Cache.
        """

        assert isinstance(user_prompt, str), "User prompt should be a string."
        assert isinstance(system_prompt, str), "System prompt should be a string."
        assert isinstance(context, str), "Context should be a string."
        assert isinstance(cache_enabled, bool), "Cache enabled should be a boolean."

        # print("LLM: =============Generating Response==============")
        # print()
        # print("System Prompt: ", system_prompt)
        # print()
        # print("Context: ", context)
        # print()
        # print("User Prompt: ", user_prompt)
        # print()

        if context != "":
            context = "Context:\n" + context + "\n"

        user_prompt = context + user_prompt

        messages = [
            ("system", system_prompt),
            ("user", user_prompt),
        ]

        messages_str = str(messages)

        response = self.__llm_cache.get_response(messages_str) if cache_enabled else None

        # if response is not None:
        #     print()
        #     print("Response from cache")
        #     print()

        if response is None:

            # print()
            # print("Response from model")
            # print()

            message = self.__client.chat_completion(
                model = self.__model,
                messages = messages,
                max_tokens = self.__max_tokens,
                temperature = self.__tempature,
                stream = False
            )
            response = message.choices[0].message.content
            if cache_enabled:
                self.__llm_cache.set_response(messages_str, response)

        # print("LLM: =============Response Generated==============")
        # print("Response: ", response)
        # print()
        # print("LLM: =============================================")
        # print()
        return response
